{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2_repnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63B_axgGuP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "03a8575a-0c25-4758-d3b7-554db7ae1246"
      },
      "source": [
        "!git clone https://github.com/rmitra/DLWorkshop-lab2\n",
        "%cd DLWorkshop-lab2\n",
        "!sh getData.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DLWorkshop-lab2' already exists and is not an empty directory.\n",
            "/content/DLWorkshop-lab2\n",
            "--2020-03-04 10:31:56--  https://www.dropbox.com/s/b20gdo2ywkbx87b/dataset_test_gt.pickle\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/b20gdo2ywkbx87b/dataset_test_gt.pickle [following]\n",
            "--2020-03-04 10:31:56--  https://www.dropbox.com/s/raw/b20gdo2ywkbx87b/dataset_test_gt.pickle\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com/cd/0/inline/AzRl_zVTLLaeuPrpt6I6YWpMNLcUd_S7epJlLvK_M3FQlcpX1L6OL-BnmcgWmWkmaCBkcqpfvpAihQbyuICXrWGLAr-1ThoKNwx-zBKouzxyXiWxgRixU5OZc1W2Fs07Nn4/file# [following]\n",
            "--2020-03-04 10:31:56--  https://uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com/cd/0/inline/AzRl_zVTLLaeuPrpt6I6YWpMNLcUd_S7epJlLvK_M3FQlcpX1L6OL-BnmcgWmWkmaCBkcqpfvpAihQbyuICXrWGLAr-1ThoKNwx-zBKouzxyXiWxgRixU5OZc1W2Fs07Nn4/file\n",
            "Resolving uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com (uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com (uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AzQa8OUXrOPYBD3zX7dmUH_5fIAO0ZsDQKjZrWrmxwTzvxhDzvBPYwXLDYFHRwLXJsu7kETCdxO7VXh3cxGO04czkXkZwn_IQDn_GGWSd28IzcMC3mi5iZi89denJPf97y336BaAm5xaRrThlXDIpJ61wAFHbilTA6M9-vYTJIoGBnmEeqkQS2W-iUa0bToRTi12xZgRZMmcASqB1ODAZDnojOACHQ3LTnZrtslNznPM5E4dW-eKynctFFgoQkq7d-74JF4_HlpjMvXFLLlwWQfoaxoi7N-4LtPnrU7qSS37ql12TBPVK_rQj_ybGi_kW_ykLMvS_bvQfZmSwWN36REJzRb7LzM3Twirn1PCArTIeg/file [following]\n",
            "--2020-03-04 10:31:57--  https://uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com/cd/0/inline2/AzQa8OUXrOPYBD3zX7dmUH_5fIAO0ZsDQKjZrWrmxwTzvxhDzvBPYwXLDYFHRwLXJsu7kETCdxO7VXh3cxGO04czkXkZwn_IQDn_GGWSd28IzcMC3mi5iZi89denJPf97y336BaAm5xaRrThlXDIpJ61wAFHbilTA6M9-vYTJIoGBnmEeqkQS2W-iUa0bToRTi12xZgRZMmcASqB1ODAZDnojOACHQ3LTnZrtslNznPM5E4dW-eKynctFFgoQkq7d-74JF4_HlpjMvXFLLlwWQfoaxoi7N-4LtPnrU7qSS37ql12TBPVK_rQj_ybGi_kW_ykLMvS_bvQfZmSwWN36REJzRb7LzM3Twirn1PCArTIeg/file\n",
            "Reusing existing connection to uc5b5ee6d234524492f0053d0eed.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59207814 (56M) [application/octet-stream]\n",
            "Saving to: ‘dataset_test_gt.pickle.1’\n",
            "\n",
            "dataset_test_gt.pic 100%[===================>]  56.46M  54.1MB/s    in 1.0s    \n",
            "\n",
            "2020-03-04 10:31:58 (54.1 MB/s) - ‘dataset_test_gt.pickle.1’ saved [59207814/59207814]\n",
            "\n",
            "--2020-03-04 10:31:58--  https://www.dropbox.com/s/hsqealoayed1grp/dataset_train_gt.pickle\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/hsqealoayed1grp/dataset_train_gt.pickle [following]\n",
            "--2020-03-04 10:31:59--  https://www.dropbox.com/s/raw/hsqealoayed1grp/dataset_train_gt.pickle\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com/cd/0/inline/AzRvvI_YkqishxkxW2Ud-ahWmYSZXy-ixsUQeZycLgB7zEqFOh9h2sf8NTb1Cixr5FJSyLzThla_0qmL4Ss62M6MMiuQcC4VRFqU0GPnG2qychG2uCNInzNKb_TERZbUdDo/file# [following]\n",
            "--2020-03-04 10:31:59--  https://uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com/cd/0/inline/AzRvvI_YkqishxkxW2Ud-ahWmYSZXy-ixsUQeZycLgB7zEqFOh9h2sf8NTb1Cixr5FJSyLzThla_0qmL4Ss62M6MMiuQcC4VRFqU0GPnG2qychG2uCNInzNKb_TERZbUdDo/file\n",
            "Resolving uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com (uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com (uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AzQNwvPV5yZsklwDbEUR2AlZA0UyfwHpA_p2t9KTXpaji80olrbxwdPVuoEO0DNvd2fGzZlZPu_19Ci1-SymDnwNSteCIwsAlRwaNVC6YvtHqI3nUzBo3a3WS8C3Pn97kCdAYJWps1XAfIfptJ_VGD8d2ouNvqOAYwUarMjKaYHBz7DZmgRoTbwNY7pp4BycuQehdg0FZIfBDG2EbSIg3MvkcB9aU41YwTB7Th6gBU7qySL4tHUQoXzE8uJIAbJdvz_lA6B-dkAhV3Nno43DaQgyc40SBFzlWfE7mHetigQgc6m8bsq5yGaa9SCQ-xjWITh4iS7MNHN2guINtpuIBfauN9ibQ9MdbyjCUBTQq4bz2g/file [following]\n",
            "--2020-03-04 10:32:00--  https://uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com/cd/0/inline2/AzQNwvPV5yZsklwDbEUR2AlZA0UyfwHpA_p2t9KTXpaji80olrbxwdPVuoEO0DNvd2fGzZlZPu_19Ci1-SymDnwNSteCIwsAlRwaNVC6YvtHqI3nUzBo3a3WS8C3Pn97kCdAYJWps1XAfIfptJ_VGD8d2ouNvqOAYwUarMjKaYHBz7DZmgRoTbwNY7pp4BycuQehdg0FZIfBDG2EbSIg3MvkcB9aU41YwTB7Th6gBU7qySL4tHUQoXzE8uJIAbJdvz_lA6B-dkAhV3Nno43DaQgyc40SBFzlWfE7mHetigQgc6m8bsq5yGaa9SCQ-xjWITh4iS7MNHN2guINtpuIBfauN9ibQ9MdbyjCUBTQq4bz2g/file\n",
            "Reusing existing connection to uc48caad8dfb58fb2c7ba8c3ca5f.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169921576 (162M) [application/octet-stream]\n",
            "Saving to: ‘dataset_train_gt.pickle.1’\n",
            "\n",
            "dataset_train_gt.pi 100%[===================>] 162.05M  46.7MB/s    in 3.5s    \n",
            "\n",
            "2020-03-04 10:32:04 (46.7 MB/s) - ‘dataset_train_gt.pickle.1’ saved [169921576/169921576]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mmQ8oB6W3Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from models import Lifter\n",
        "# from models import Critic\n",
        "from PoseDataset import PoseDataset\n",
        "from utils import MPJPE\n",
        "from utils import project_to_2d\n",
        "from arguments import parse_args\n",
        "from tqdm import tqdm\n",
        "import easydict\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEI_kvck1dgn",
        "colab_type": "text"
      },
      "source": [
        "# Implementing RepNet using PyTorch\n",
        "## Overview\n",
        "In this lab, we'll continue from the previous discussions on 3D Human Pose Estimation and try to implement a weakly-supervised method for human pose estimation - Repnet. The method relies on 'reprojection loss' between 2D and 3D poses and is aided by adversarial training.\n",
        "\n",
        "<div>\n",
        "    <img src='https://www.cse.iitb.ac.in/~rdabral/dl_workshop/repnet.png' width = 600>\n",
        "</div>\n",
        "\n",
        "The method subjects the regressed 3D poses to two main losses:\n",
        "<ul>\n",
        "    <li> <b>Reprojection Loss</b>: The poses are projected to 2D using learnt camera parameters. The projected 2D poses are then compared with the input 2D poses. For the purpose of this lab, <b>we'll avoid 'learning' the camera parameters and provide them explicitly</b>.\n",
        "    <li> <b>Critic Loss</b>: The Critic/Adversarial loss will discriminate between real (gt) and fake (generated) 3D poses. We'll train this with Wasserstein Loss\n",
        "</ul>\n",
        "\n",
        "## Model Construction\n",
        "We begin by defining and constructing the network architecture. The network comprises of two separate networks: \n",
        "<ul>\n",
        "    <li> <b>Lifter</b>: The Lifter network takes as input a batch of 2D poses and converts them to 3D poses. We're calling it 'Lifter' because it is, in essence, <i>lifting</i> the poses from 2D to 3D.\n",
        "    <li> <b>Critic</b>: The Critic network takes as input a batch of real and fake 3D poses and outputs a 1-d scalar representing whether the poses are real or fake.\n",
        "</ul>\n",
        "\n",
        "The Lifter and Critic network architectures look like this:\n",
        "<div>\n",
        "    <img src='https://www.cse.iitb.ac.in/~rdabral/dl_workshop/Architecture.png' width = 600>\n",
        "</div>\n",
        "\n",
        "We'll start by first writing the model architecture using the nn.Module class we learned about yesterday. You're being provided with the ResNet block pre-implemented. We have also written the constructor for the Lifter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB5TmWiQ2TLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.l1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.l2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.l1(x)\n",
        "        y = self.leaky_relu(y)\n",
        "        y = self.l2(y)\n",
        "        z = x + y\n",
        "        z = self.leaky_relu(z)\n",
        "        return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw7zcp303iB0",
        "colab_type": "text"
      },
      "source": [
        "We have also written the constructor for the Lifter model. Your task now is to write the forward of the Lifter model. The input to the Lifter is a 2\\*17-dimensional vector and the output is a 3\\*17-dimensional vector, just like yesterday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGa1ceTn5CmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lifter(nn.Module):\n",
        "    def __init__(self, num_joints, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.num_joints = num_joints\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # The next blocks are meant for 3D pose regression\n",
        "        self.pose_reg = nn.Sequential(\n",
        "                        nn.Linear(2*self.num_joints, self.hidden_dim),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        ResBlock(self.hidden_dim),\n",
        "                        ResBlock(self.hidden_dim),\n",
        "                        ResBlock(self.hidden_dim),\n",
        "                        nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.Linear(self.hidden_dim, 3*self.num_joints)\n",
        "                        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: b x n_joints*2\n",
        "        # poses_3d = \n",
        "\n",
        "        return self.pose_reg(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqear_H-ICsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e9fc961-3803-4e59-9bd1-07776f45de1e"
      },
      "source": [
        "Lifter(17,1000)(torch.rand(1,34)).size()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 51])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3dlbF2d6thv",
        "colab_type": "text"
      },
      "source": [
        "Hope this was easy! Notice that we can use nn.Sequential() to keep our code inside the forward function consize and neat (for the sequential parts, of course). Now, you need to implement the Critic network on your own. The input to the Critic will be a 3D pose: 3*17 dimensional vector and output will be a scalar (1-d)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h1BCrce6_An",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, num_joints, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.num_joints = num_joints\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.l1 = nn.Linear(3*num_joints, self.hidden_dim)\n",
        "        self.act=nn.LeakyReLU(0.1)\n",
        "        self.res=ResBlock(self.hidden_dim)\n",
        "        self.l2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.l3 = nn.Linear(self.hidden_dim, 1)\n",
        "        self.sig=nn.Sigmoid()\n",
        "        # Initialize your network layers here\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward propagate the input through your network layers here\n",
        "        out = self.act(self.l1(input))\n",
        "        out = self.l3(self.act(self.res(self.l2(out))))\n",
        "        return self.sig(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2bCxWSAIROR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1eccd7f6-575c-40d5-967b-06803b95ae11"
      },
      "source": [
        "Critic(17,1000)(torch.rand(1,51)).size()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DmLdN-u7dxN",
        "colab_type": "text"
      },
      "source": [
        "Now that we're done defining the models, let's jump into some rountine stuffs. We'll initialize the two models and push them to cuda. We'll also initialize two optimizers, one for each model. Why do we need different optimizers for the two? Because GANs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIdvCZjxXgke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = easydict.EasyDict({'batch_size': 32,\n",
        "        'epochs': 10,\n",
        "        'learning_rate': 0.00005,})\n",
        "\n",
        "lifter = Lifter(num_joints=17, hidden_dim=1000)\n",
        "lifter.cuda()\n",
        "lifter_optimizer = torch.optim.Adam(lifter.parameters(),\n",
        "                                        lr=cfg.learning_rate,\n",
        "                                        betas=(0.5, 0.9)\n",
        "                                        )\n",
        "\n",
        "# Initializing the adversarial/critic module\n",
        "critic = Critic(num_joints=17, hidden_dim=100)\n",
        "critic.cuda()\n",
        "\n",
        "# We think Adam optimizer should work well for our critic. But you are\n",
        "# encouraged to explore other ones, say RMSprop, or SGD. Google!\n",
        "critic_optimizer = torch.optim.Adam(critic.parameters(),\n",
        "                                        lr=cfg.learning_rate,\n",
        "                                        betas=(0.5, 0.9)\n",
        "                                        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2hAK0dQ8okB",
        "colab_type": "text"
      },
      "source": [
        "The next piece of the jigsaw is the data. This is complex; probably, the clumsiest part of any deep learning code! We have implemented a DataSet class for you. <b>Ask questions if you're interested. Or, we can treat it as a blackbox.</b> The dataset object is given to PyTorch's dataloader class whose sole job is to efficiently churn out nicely arranged batches of data for you. Let's try to see what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73AUdQhZQ5I",
        "colab_type": "code",
        "outputId": "bc0a6e3f-4569-4e1f-815d-7facc97a594e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Data Loaders\n",
        "train_dataset = PoseDataset(cfg, split='train')\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size = cfg.batch_size,\n",
        "                                shuffle = True,\n",
        "                                num_workers = 4\n",
        "                                )\n",
        "\n",
        "# train_loader is an iterator. We can use enumerate to iterate over it:\n",
        "for i, (pose_2d, pose_3d_real, cams, roots) in enumerate(train_loader):\n",
        "  # This is the input to the Lifter Network\n",
        "  print(pose_2d.shape)      \n",
        "  # These are some random 3D poses. They'll be used to train the Critic.\n",
        "  print(pose_3d_real.shape)\n",
        "  # These are the camera parameters. They'll be used to project the predicted\n",
        "  # 3D poses back to 2D image coordinates. \n",
        "  print(cams.shape)\n",
        "  # Ignore this. These are also used for reprojection.\n",
        "  print(roots.shape)\n",
        "  break\n",
        "\n",
        "test_dataset = PoseDataset(cfg, split='test')\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                                test_dataset,\n",
        "                                batch_size = cfg.batch_size,\n",
        "                                shuffle = True,\n",
        "                                num_workers = 4\n",
        "                                )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loaded 312188 samples\n",
            "torch.Size([32, 34])\n",
            "torch.Size([32, 51])\n",
            "torch.Size([32, 9])\n",
            "torch.Size([32, 1, 3])\n",
            "\n",
            " Loaded 108772 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWKaM90__2iB",
        "colab_type": "text"
      },
      "source": [
        "We're done with the models, optimizers and the data. The last things we need are the loss functions. We'll discuss their use in detail later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRKCo2mZV7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rep_loss = nn.MSELoss().cuda()\n",
        "crit_loss = nn.BCELoss().cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws66fP_iH4pm",
        "colab_type": "text"
      },
      "source": [
        "Let's define the training loop now. For the purposes of this code, we call the train() function for every epoch. The dataloader iterates over all the batches of the dataset withing this function. The training iteration will follow the following algorithm:\n",
        "<ul>\n",
        "  <li> Forward prop the 2D input through the Lifter to get the 3D pose predictions.\n",
        "  <li> Train the Critic:\n",
        "        <ul>\n",
        "          <li> Forward prop the 3D predictions (fake) through the Critic\n",
        "          <li> Forward prop the Real 3D poses through the Critic. Note that these 'real' 3D poses have no relation with the predicted (or fake) 3D poses. They've simply been sampled from the training dataset.\n",
        "          <li> Compute and backpropagate the Wasserstein-loss on the real and fake outputs. Note, that <b>we do not want to backprop the Lifter model with this loss</b>. To avoid that from happening, use detach().\n",
        "        </ul>\n",
        "  <li> Train the Lifter:\n",
        "        <ul>\n",
        "          <li> Forward prop the 3D predictions through the Critic.\n",
        "          <li> Compute the Wasserstein-loss on the predicted poses. This time, the loss needs to be backpropagated to the Lifter model as well. The higher this loss, the better our Lifter! Why?\n",
        "          <li> Use the camera parameters provided by the data-loader to project the 3D pose predictions into 2D poses. \n",
        "          <li> Compute the reprojection loss. It is the MSE loss between the input 2D poses (to the Lifter) and the projected 2D poses (from the Lifter).\n",
        "          <li> Add the critic and reprojection losses and backpropagate them through the lifter module. Update the Lifter weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_z3v3EsdOMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, lifter, critic, lifter_optimizer, critic_optimizer,\n",
        "                                rep_loss, crit_loss, train_loader):\n",
        "\n",
        "    # train\n",
        "    rep_loss_total, lifter_critic_loss_total = 0, 0\n",
        "    discriminator_loss_total = 0\n",
        "    train_loader.dataset.init_epoch()\n",
        "\n",
        "    for i, (input_2d, real_poses_3d, cameras, root) in enumerate(tqdm(train_loader, ascii=True)):\n",
        "        # Constant Tensors to be used as labels for losses\n",
        "        valid = torch.ones((real_poses_3d.size(0), 1)).float().cuda()\n",
        "        fake = torch.zeros((input_2d.size(0), 1)).float().cuda()\n",
        "\n",
        "        input_2d = input_2d.cuda()\n",
        "        real_poses_3d = real_poses_3d.cuda()\n",
        "        cameras = cameras.cuda()\n",
        "        root = root.cuda()\n",
        "        poses_3d = lifter(input_2d)\n",
        "\n",
        "        # ---------------- Training the Critic ------------------\n",
        "\n",
        "        real_validity = critic(real_poses_3d)\n",
        "        fake_validity = critic(poses_3d.detach())\n",
        "        # Wasserstein-loss for the Critic\n",
        "\n",
        "        #when we pass in real image we want outputs to be close to 1 i.e as high as possible and vice versa for fake *****\n",
        "        # so for real validity we want to increase the output and for fake decrease\n",
        "        # so max real_out + (-fake_out)\n",
        "        # For torch's backward: \n",
        "        critic_loss = -torch.mean(real_validity) + torch.mean(fake_validity) \n",
        "        # this is wassertein so we just act in a raw way on these outputs\n",
        "        # Generally we would do this for the cross-entropy output:\n",
        "        # Traditional GAN loss\n",
        "        # critic_loss = 0.5 * (crit_loss(real_validity, valid) + crit_loss(fake_validity, fake))\n",
        "        # this includes the ce loss with all 1's and 0's labels\n",
        "\n",
        "        \n",
        "        # Flush the existing gradients and backpropagate the loss\n",
        "        critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "        #  ------------- Training the Generator -----------------\n",
        "\n",
        "        # Wasserstein Loss for the Lifter\n",
        "        lifter_critic_loss = -torch.mean(critic(poses_3d))\n",
        "        # Traditional GAN loss\n",
        "        # lifter_critic_loss = nn.crit_loss(critic(poses_3d), valid)\n",
        "\n",
        "        # Reprojection loss\n",
        "        poses_rep = project_to_2d(poses_3d, cameras, root)\n",
        "        reprojection_loss = rep_loss(poses_rep, input_2d)\n",
        "\n",
        "        # Combined loss\n",
        "        lifter_loss = reprojection_loss + lifter_critic_loss\n",
        "\n",
        "        # Flush the existing gradients and backpropagate the loss\n",
        "        lifter_optimizer.zero_grad()\n",
        "        lifter_loss.backward()\n",
        "        lifter_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # --------------- Book Keeping --------------------------\n",
        "        rep_loss_total += reprojection_loss.cpu().item()\n",
        "        lifter_critic_loss_total += lifter_critic_loss.cpu().item()\n",
        "        discriminator_loss_total += critic_loss.cpu().item()\n",
        "\n",
        "    print(f'epoch: {epoch}, reprojection_loss: {rep_loss_total / (i+1) :.6f}, \\\n",
        "            lifter_critic_loss = {lifter_critic_loss_total / (i+1) :.4f}, \\\n",
        "            discriminator_critic_loss = {discriminator_loss_total / (i+1) :.6f}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ywMr0eL10M",
        "colab_type": "text"
      },
      "source": [
        "We're implementing a function to validate the performance of our Lifter model. This will compute the Mean Per Joint Position Error at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q70MtdvdVtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val(epoch, lifter, test_loader):\n",
        "    # Setting the model's mode to eval mode\n",
        "    lifter.eval()\n",
        "\n",
        "    mpjpe = 0\n",
        "    for i, (input_2d, gt_3d, cameras) in enumerate(test_loader):\n",
        "        input_2d = input_2d.cuda()\n",
        "        gt_3d = gt_3d.cuda()\n",
        "        cameras = cameras.cuda()\n",
        "\n",
        "        pred_3d = lifter(input_2d)\n",
        "\n",
        "        # Calculate the Mean Per Joint Position Error\n",
        "        mpjpe += MPJPE(pred_3d, gt_3d)\n",
        "\n",
        "    print(f\"The MPJPE for epoch {epoch} is {mpjpe/i}\")\n",
        "\n",
        "    # Resetting the model back to training mode\n",
        "    lifter.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SCVu4QZMBBd",
        "colab_type": "text"
      },
      "source": [
        "Let's train the entire thing for 10 epochs and see what happens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_3BkT1_g75-",
        "colab_type": "code",
        "outputId": "7b64bd1a-51b1-4d38-a2ec-36641796d393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# Enter the training loop\n",
        "for epoch in range(cfg.epochs):\n",
        "    train(epoch, lifter, critic, lifter_optimizer, critic_optimizer,\n",
        "                            rep_loss, crit_loss, train_loader)\n",
        "\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        val(epoch, lifter, test_loader)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:36<00:00, 45.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, reprojection_loss: 0.012230,             lifter_critic_loss = -0.0479,             discriminator_critic_loss = 0.006636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 0 is 128.42086791992188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:41<00:00, 44.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, reprojection_loss: 0.000149,             lifter_critic_loss = -0.0158,             discriminator_critic_loss = 0.000132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 1 is 114.85527801513672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:40<00:00, 45.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 2, reprojection_loss: 0.000100,             lifter_critic_loss = -0.0081,             discriminator_critic_loss = 0.000070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 2 is 120.7652359008789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:40<00:00, 45.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 3, reprojection_loss: 0.000098,             lifter_critic_loss = -0.0030,             discriminator_critic_loss = 0.000032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 3 is 110.8995590209961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:38<00:00, 45.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 4, reprojection_loss: 0.000125,             lifter_critic_loss = -0.0019,             discriminator_critic_loss = -0.000033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 4 is 112.99685668945312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:41<00:00, 44.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 5, reprojection_loss: 0.000181,             lifter_critic_loss = -0.0047,             discriminator_critic_loss = -0.000170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9756 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The MPJPE for epoch 5 is 123.29306030273438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 9756/9756 [03:45<00:00, 43.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 6, reprojection_loss: 0.000321,             lifter_critic_loss = -0.0135,             discriminator_critic_loss = -0.000572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8EWBUPBUunm",
        "colab_type": "text"
      },
      "source": [
        "Voila! You just implemented a CVPR paper :-)"
      ]
    }
  ]
}